{"pageProps":{"post":{"title":" 实现线上服务低延迟","comments":true,"date":"2019-09-24T09:14:01.000Z","tags":["读书","总结","Google"],"categories":["读书"],"excerpt":"原文标题：Achieving Rapid Response Times in Large Online Services","modifyTime":"2021-03-27 14:49:15 +0800","createTime":"2021-03-27 14:49:15 +0800","logs":[{"status":["A"],"files":["\"2021-03-27 \\345\\256\\236\\347\\216\\260\\347\\272\\277\\344\\270\\212\\346\\234\\215\\345\\212\\241\\344\\275\\216\\345\\273\\266\\350\\277\\237.md\""],"abbrevHash":"f4f4061","hash":"f4f406175bde5279c552d8bac1c05cf7bb72d58e","subject":"Copy old posts","authorName":"yajw","authorDate":"2021-03-27 14:49:15 +0800"}],"link":"2021-03-27%20%E5%AE%9E%E7%8E%B0%E7%BA%BF%E4%B8%8A%E6%9C%8D%E5%8A%A1%E4%BD%8E%E5%BB%B6%E8%BF%9F","id":"2de3aab8-522a-478f-81a9-979ff1fe6960","content":"<p>这个ppt是2012年3月份，那时我大三下学期，刚好接触ACM吧。</p>\n<p>很多大型系统都按照模块拆分成很多个后台服务，构成扇形结构（Fanout Services）。</p>\n<p>这种结构下，一个顶层的请求会被拆分成很多对子系统的请求，这时就如水桶原理，整个请求的延时取决于最慢的那个。这里面是个概率分布问题，一个顶层请求分发给越多的机器，那么相应时间也就越可能变慢。假设单个机器的响应时间分布是独立的，考虑多台机器的相应时间的联合分布。</p>\n<p>按照这个ppt所说，Google的机器是多个服务共享的，也就是一个机器可能部署很多个各种各样的服务。这个类似云计算的资源共享，能提升资源利用率，降低成本。但是对一个服务来说，这个环境的相应时间有些不可预测，因为需要去竞争有限的资源。那么云计算是如何解决资源调度的问题呢？这个问题等研究后再说。</p>\n<p>相对于Fault Tolerating，这个ppt提出Variability Tolerating。Fault Tolerating是用多余的不可靠的组件来构建可靠的系统。variability Tolerating是用不可预测的部件来构造可预测的系统。提到了一些排队机制，精细地管理机器上的活动，更多是系统层面的。</p>\n<p>下面才是干货，Latency Tolerating Techniques：</p>\n<ol>\n<li>服务层面的自适应\n<ul>\n<li>收集相应指标，尽可能减少后面请求的延时</li>\n<li>具体措施，动态负载均衡机制，例如熔断器之类的</li>\n<li>分片：把服务分布在多台机器上</li>\n<li>有选择备份：对重要的或者热点加更多的备份</li>\n</ul>\n</li>\n<li>请求层面的自适应，一些pattern\n<ul>\n<li>canary request: 尝试一个节点没问题，概率上对其他节点就有信心</li>\n<li>backup requests：对付长尾效应，等2ms再发一个请求，并且在收到回复后取消另一个请求（降低资源消耗？）给出的数据还是蛮有说服力的。这里有个前提是请求要是幂等的。</li>\n<li>根据业务需求，tradeoff准确性和延迟</li>\n</ul>\n</li>\n</ol>\n<p>读后感：方法很多，更多地要靠实践，具体问题具体分析，用好的结果来回答怎么降低延迟的问题，这个ppt整体也是这样的思路。</p>\n"}},"__N_SSG":true}