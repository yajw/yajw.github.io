{"pageProps":{"post":{"title":" 2019-12-20 linux 实用命令","comments":true,"tags":["linux","awk","split","sed"],"categories":["实用"],"modifyTime":"2021-03-27 14:49:15 +0800","createTime":"2021-03-27 14:49:15 +0800","logs":[{"status":["A"],"files":["\"2021-03-27 2019-12-20 linux \\345\\256\\236\\347\\224\\250\\345\\221\\275\\344\\273\\244.md\""],"abbrevHash":"f4f4061","hash":"f4f406175bde5279c552d8bac1c05cf7bb72d58e","subject":"Copy old posts","authorName":"yajw","authorDate":"2021-03-27 14:49:15 +0800"}],"link":"2021-03-27%202019-12-20%20linux%20%E5%AE%9E%E7%94%A8%E5%91%BD%E4%BB%A4","id":"99ae7def-8551-4a5e-af12-1908a68c2aef","content":"<h2>sftp</h2>\n<p>upload local file to remote sftp:</p>\n<pre><code class=\"language-bash\">sftp -oPort=2201 user@host:${remote_path} &#x3C;&#x3C;&#x3C; $'put {local_file_path}'\n</code></pre>\n<p>download remote file to local:</p>\n<pre><code class=\"language-bash\">sftp user@host:${remote_path} ${local_path}\n</code></pre>\n<p>路径支持通配符来匹配多个文件</p>\n<p>sftp常用命令有这些：<code>ls</code> <code>ls -lh</code> <code>rm</code> <code>put</code> <code>get</code>等。</p>\n<h2>split</h2>\n<ul>\n<li>mac安装gsplit: <code>brew install coreutils</code></li>\n</ul>\n<p>文件分割比较有用。例如把一个script.sql文件按每100行拆分，输出文件前缀为split_\nmac上用的gsplit (GNU版本的split)</p>\n<pre><code class=\"language-bash\">gsplit -dl 400 --additional-suffix=.sql script.sql script_\n</code></pre>\n<p>也支持按文件大小分割，同时支持按行分割</p>\n<pre><code class=\"language-bash\">gsplit -C 10M --additional-suffix=.sql script.sql script_\n</code></pre>\n<h2>sed</h2>\n<p>例如，script.sql文件每行一条insert语句，想要改成<code>on duplicate update</code>来同时支持插入和已有记录的更新（这个操作是幂等的），就可以用下面的方法：</p>\n<p>第一步，去掉文件每行末的最后一个字符：</p>\n<pre><code class=\"language-bash\">sed 's/.$//' script.sql > script_noprefix.sql\n</code></pre>\n<p>然后，给每行加上指定字符串，这里是<code>ON DUPLICATE KEY</code>:</p>\n<pre><code class=\"language-bash\">sed 's/.$/ ON DUPLICATE KEY UPDATE update_time=values(update_time);/' script_noprefix.sql > script_update_insert.sql\n</code></pre>\n<p>sed支持直接替换，上面的例子也可以直接替最后一个字符为指定字符串。\n一个替换的例子(mac版本)，<strong>注意：这里是原地替换，直接修改原文件内</strong>：</p>\n<pre><code class=\"language-bash\">sed -i '' -- 's/.$/ ON DUPLICATE KEY UPDATE update_time=values(update_time);/g' script.sql\n</code></pre>\n<h2>awk 实用套路</h2>\n<p>其他学习文章</p>\n<ul>\n<li>AWK 简明教程 https://coolshell.cn/articles/9070.html</li>\n<li>awk 入门教程 http://www.ruanyifeng.com/blog/2018/11/awk.html</li>\n</ul>\n<h3>匹配</h3>\n<p><strong>包含某些关键词，但不包含其他关键词</strong></p>\n<pre><code class=\"language-bash\">awk -F '|' '/error|warn/ &#x26;&#x26; !/system/' error.log\n</code></pre>\n<h3>统计</h3>\n<p><strong>日志第一列是ip，数不同ip的日志行数</strong></p>\n<pre><code class=\"language-bash\">awk -v OFS='\\t' -F '|' '{count[$1]++;} END {for (ip in count) print ip, count[ip]}' info.log\n</code></pre>\n<p><strong>按照ip数统计某个url的请求量</strong></p>\n<pre><code class=\"language-bash\">awk -F '|' '/api\\/service/ &#x26;&#x26; !/api\\/service\\/other/ {c[$1]++} END {for (ip in c) print ip,c[ip]}' info.log\n</code></pre>\n<p><strong>按照小时统计某个url的请求量</strong></p>\n<p>sample log</p>\n<pre><code>0.0.0.0|2019-12-19 13:01:02|elapsed=12ms,url=/api/service\n</code></pre>\n<pre><code class=\"language-bash\">awk -F '|' '/api\\/service/ {split($2, t, \"[-: ]\"); c[t[4]]++;} END {for (hour in c) print hour,c[hour]}' info.log | sort -k1\n</code></pre>\n<p><strong>按照小时统计某个url的请求量，并且包含占总量的百分比</strong></p>\n<pre><code class=\"language-bash\">awk -F '|' '/api\\/service/ {split($2, t, \"[-: ]\"); c[t[4]]++; s++} END {for (hour in c) printf \"%s\\t%s\\t%.2f\\n\" hour,c[hour],100*c[hour]/s}' info.log | sort -k1\n</code></pre>\n<p><strong>统计错误日志中不同类型的种类</strong></p>\n<pre><code class=\"language-bash\">awk -F '|' ' /ERROR/ {c[$7]++;s++} END {for (r in c) printf \"%s\\t%.2f\\t%s\\n\", c[r],100*c[r]/s,r}' error.log | sort -k2\n</code></pre>\n<p><strong>连接状态统计</strong></p>\n<pre><code class=\"language-bash\">ss | awk 'NR!=1{c[$2]++} END {for (s in c) print s, c[s]}'\n</code></pre>\n<p><strong>统计不同目的ip的不同状态的连接数</strong></p>\n<pre><code class=\"language-bash\">netstat -t | awk -v OFS='\\t\\t' 'NR>2{c[$5,$6]++} END {for (x in c) {split(x, s, SUBSEP); print c[x],s[2],s[1]} }'\n</code></pre>\n<h3>远程awk重定向输出到本地文件</h3>\n<p>这个需要在本地分析日志时很有用，heredoc可以避开escape的问题。</p>\n<p>借助heredoc和ssh重定向</p>\n<pre><code class=\"language-bash\">ssh > daemon.log aps-live-log &#x3C;&#x3C;-'EOF'\nawk -F '|' '!/ktc_settlement_report/ &#x26;&#x26; !/txn_3ds/ &#x26;&#x26; $2>\"[2020-01-05 01:00:00\" &#x26;&#x26; $2&#x3C;\"[2020-01-05 01:05:00\"' /data/error.log\nEOF\n</code></pre>\n<h3>awk 正则匹配筛选比较耗时的请求</h3>\n<p>正则匹配提取：</p>\n<pre><code class=\"language-bash\">awk -F'|' 'match($6, /elapsed=([0-9]+)/, ta) &#x26;&#x26; match($6, /id=([0-9]+)/, ka) {c[ka[1]]=ta[1]} END {for (k in c) {print t,c[t]}}' data.log | sort -n -k 2\n</code></pre>\n<p>数值比较（筛选出耗时超过2秒的请求）</p>\n<pre><code class=\"language-bash\"> awk -F'|' 'match($6, /elapsed=([0-9]+)/, ta) &#x26;&#x26; match($6, /id=([0-9]+)/, ka) {if ((ta[1]+0)>2000) c[ka[1]]=ta[1]} END {for (t in c) {print t,c[t]}}' data_log | sort -n -k 2\n</code></pre>\n<h3>ss</h3>\n<p>mac 下模拟ss: <code>alias ss='lsof -Pn -i4 | grep LISTEN'</code></p>\n<h3>ssh</h3>\n<p>ssh 转发： https://jin-yang.github.io/post/ssh-proxy.html</p>\n<h3>获取访问某个ip的本地接口</h3>\n<p>outgoing ip</p>\n<p>ip route get 8.8.8.8\nmac: brew install iproute2mac</p>\n"}},"__N_SSG":true}