<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>blog -  实现线上服务低延迟</title><link rel="icon" href="/favicon.ico"/><link rel="stylesheet" href="/static/css/androidstudio.min.css"/><script src="/static/js/highlight.min.js"></script><script>hljs.highlightAll();</script><meta name="next-head-count" content="7"/><link rel="preload" href="/_next/static/css/3c5946e20da2cc20d075.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3c5946e20da2cc20d075.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e2e6dc732d39303ab748.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e2e6dc732d39303ab748.css" data-n-p=""/><link rel="preload" href="/_next/static/css/4b5ef9ffeb729d4f1c31.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4b5ef9ffeb729d4f1c31.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-c006ee6087559bbd65e3.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.15878d4e523f86636251.js" as="script"/><link rel="preload" href="/_next/static/chunks/99f422a92ff7083adb8a7d840734144fa7589f68.e37cca09058b1656d546.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-86af5e0fcdd51c9c3a66.js" as="script"/><link rel="preload" href="/_next/static/chunks/314642ff.81d3755a1df95fed9f2f.js" as="script"/><link rel="preload" href="/_next/static/chunks/762e22088df2ca7ea97d3f731b2a7315c482f91e.cb9acb13aef2b3683e00.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/blog/%5Blink%5D-87cd50cf458654f0c030.js" as="script"/></head><body><div id="__next"><div class="Home_container__1EcsU"><main class="Home_main__1x8gC"><div class="ui text container"><h1 class="ui header"> 实现线上服务低延迟</h1><span></span><section><p>这个ppt是2012年3月份，那时我大三下学期，刚好接触ACM吧。</p>
<p>很多大型系统都按照模块拆分成很多个后台服务，构成扇形结构（Fanout Services）。</p>
<p>这种结构下，一个顶层的请求会被拆分成很多对子系统的请求，这时就如水桶原理，整个请求的延时取决于最慢的那个。这里面是个概率分布问题，一个顶层请求分发给越多的机器，那么相应时间也就越可能变慢。假设单个机器的响应时间分布是独立的，考虑多台机器的相应时间的联合分布。</p>
<p>按照这个ppt所说，Google的机器是多个服务共享的，也就是一个机器可能部署很多个各种各样的服务。这个类似云计算的资源共享，能提升资源利用率，降低成本。但是对一个服务来说，这个环境的相应时间有些不可预测，因为需要去竞争有限的资源。那么云计算是如何解决资源调度的问题呢？这个问题等研究后再说。</p>
<p>相对于Fault Tolerating，这个ppt提出Variability Tolerating。Fault Tolerating是用多余的不可靠的组件来构建可靠的系统。variability Tolerating是用不可预测的部件来构造可预测的系统。提到了一些排队机制，精细地管理机器上的活动，更多是系统层面的。</p>
<p>下面才是干货，Latency Tolerating Techniques：</p>
<ol>
<li>服务层面的自适应
<ul>
<li>收集相应指标，尽可能减少后面请求的延时</li>
<li>具体措施，动态负载均衡机制，例如熔断器之类的</li>
<li>分片：把服务分布在多台机器上</li>
<li>有选择备份：对重要的或者热点加更多的备份</li>
</ul>
</li>
<li>请求层面的自适应，一些pattern
<ul>
<li>canary request: 尝试一个节点没问题，概率上对其他节点就有信心</li>
<li>backup requests：对付长尾效应，等2ms再发一个请求，并且在收到回复后取消另一个请求（降低资源消耗？）给出的数据还是蛮有说服力的。这里有个前提是请求要是幂等的。</li>
<li>根据业务需求，tradeoff准确性和延迟</li>
</ul>
</li>
</ol>
<p>读后感：方法很多，更多地要靠实践，具体问题具体分析，用好的结果来回答怎么降低延迟的问题，这个ppt整体也是这样的思路。</p>
</section><div role="list" class="ui list"><div role="listitem" class="item">2021-03-27 14:49:15 +0800<!-- --> <!-- -->yajw<!-- --> <!-- -->Copy old posts<!-- --> <!-- -->A</div></div><div id="comments"></div></div></main><footer class="Home_footer__1WdhD"><div class="Home_powered__2GdUA">Powered by<!-- --> <a href="https://nextjs.org/" target="_blank" rel="noopener noreferrer">Next.js</a>.</div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":" 实现线上服务低延迟","comments":true,"date":"2019-09-24T09:14:01.000Z","tags":["读书","总结","Google"],"categories":["读书"],"excerpt":"原文标题：Achieving Rapid Response Times in Large Online Services","modifyTime":"2021-03-27 14:49:15 +0800","createTime":"2021-03-27 14:49:15 +0800","logs":[{"status":["A"],"files":["\"2021-03-27 \\345\\256\\236\\347\\216\\260\\347\\272\\277\\344\\270\\212\\346\\234\\215\\345\\212\\241\\344\\275\\216\\345\\273\\266\\350\\277\\237.md\""],"abbrevHash":"f4f4061","hash":"f4f406175bde5279c552d8bac1c05cf7bb72d58e","subject":"Copy old posts","authorName":"yajw","authorDate":"2021-03-27 14:49:15 +0800"}],"link":"2021-03-27%20%E5%AE%9E%E7%8E%B0%E7%BA%BF%E4%B8%8A%E6%9C%8D%E5%8A%A1%E4%BD%8E%E5%BB%B6%E8%BF%9F","id":"2de3aab8-522a-478f-81a9-979ff1fe6960","content":"\u003cp\u003e这个ppt是2012年3月份，那时我大三下学期，刚好接触ACM吧。\u003c/p\u003e\n\u003cp\u003e很多大型系统都按照模块拆分成很多个后台服务，构成扇形结构（Fanout Services）。\u003c/p\u003e\n\u003cp\u003e这种结构下，一个顶层的请求会被拆分成很多对子系统的请求，这时就如水桶原理，整个请求的延时取决于最慢的那个。这里面是个概率分布问题，一个顶层请求分发给越多的机器，那么相应时间也就越可能变慢。假设单个机器的响应时间分布是独立的，考虑多台机器的相应时间的联合分布。\u003c/p\u003e\n\u003cp\u003e按照这个ppt所说，Google的机器是多个服务共享的，也就是一个机器可能部署很多个各种各样的服务。这个类似云计算的资源共享，能提升资源利用率，降低成本。但是对一个服务来说，这个环境的相应时间有些不可预测，因为需要去竞争有限的资源。那么云计算是如何解决资源调度的问题呢？这个问题等研究后再说。\u003c/p\u003e\n\u003cp\u003e相对于Fault Tolerating，这个ppt提出Variability Tolerating。Fault Tolerating是用多余的不可靠的组件来构建可靠的系统。variability Tolerating是用不可预测的部件来构造可预测的系统。提到了一些排队机制，精细地管理机器上的活动，更多是系统层面的。\u003c/p\u003e\n\u003cp\u003e下面才是干货，Latency Tolerating Techniques：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e服务层面的自适应\n\u003cul\u003e\n\u003cli\u003e收集相应指标，尽可能减少后面请求的延时\u003c/li\u003e\n\u003cli\u003e具体措施，动态负载均衡机制，例如熔断器之类的\u003c/li\u003e\n\u003cli\u003e分片：把服务分布在多台机器上\u003c/li\u003e\n\u003cli\u003e有选择备份：对重要的或者热点加更多的备份\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e请求层面的自适应，一些pattern\n\u003cul\u003e\n\u003cli\u003ecanary request: 尝试一个节点没问题，概率上对其他节点就有信心\u003c/li\u003e\n\u003cli\u003ebackup requests：对付长尾效应，等2ms再发一个请求，并且在收到回复后取消另一个请求（降低资源消耗？）给出的数据还是蛮有说服力的。这里有个前提是请求要是幂等的。\u003c/li\u003e\n\u003cli\u003e根据业务需求，tradeoff准确性和延迟\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e读后感：方法很多，更多地要靠实践，具体问题具体分析，用好的结果来回答怎么降低延迟的问题，这个ppt整体也是这样的思路。\u003c/p\u003e\n"}},"__N_SSG":true},"page":"/blog/[link]","query":{"link":"2021-03-27 实现线上服务低延迟"},"buildId":"ravzvVxRlFE3_IAU4jdd7","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-ff94e68042added27a93.js"></script><script src="/_next/static/chunks/main-c006ee6087559bbd65e3.js" async=""></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.15878d4e523f86636251.js" async=""></script><script src="/_next/static/chunks/99f422a92ff7083adb8a7d840734144fa7589f68.e37cca09058b1656d546.js" async=""></script><script src="/_next/static/chunks/pages/_app-86af5e0fcdd51c9c3a66.js" async=""></script><script src="/_next/static/chunks/314642ff.81d3755a1df95fed9f2f.js" async=""></script><script src="/_next/static/chunks/762e22088df2ca7ea97d3f731b2a7315c482f91e.cb9acb13aef2b3683e00.js" async=""></script><script src="/_next/static/chunks/pages/blog/%5Blink%5D-87cd50cf458654f0c030.js" async=""></script><script src="/_next/static/ravzvVxRlFE3_IAU4jdd7/_buildManifest.js" async=""></script><script src="/_next/static/ravzvVxRlFE3_IAU4jdd7/_ssgManifest.js" async=""></script></body></html>